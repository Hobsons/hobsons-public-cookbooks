{
  "dependencies": {
    "apt": ">= 0.0.0"
  },
  "name": "flume_infochimps/",
  "maintainer_email": "coders@infochimps.com",
  "attributes": {
    "flume/data_dir": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "/data/db/flume",
      "type": "string",
      "recipes": [

      ],
      "display_name": "Directory for local in-transit files",
      "description": "Directory for local in-transit files"
    },
    "flume/cluster_name": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "cluster_name",
      "type": "string",
      "recipes": [

      ],
      "display_name": "",
      "description": "The name of the cluster to participate with (masters and zookeepers...)"
    },
    "flume/plugins": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "",
      "type": "string",
      "recipes": [

      ],
      "display_name": "Hash for plugin configuration",
      "description": "If you have a particular plugin to configure, you can also configure the classpath and the classes to include in the configuration file with attributes in the following forms:\nnode[:flume][:plugin][{plugin_name}][:classes]\nnode[:flume][:plugin][{plugin_name}][:classpath]\nnode[:flume][:plugin][{plugin_name}][:java_opts]"
    },
    "flume/master/zookeeper_port": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "2181",
      "type": "string",
      "recipes": [

      ],
      "display_name": "port to talk to zookeeper on (for external zookeeper)",
      "description": "port to talk to zookeeper on (for external zookeeper)"
    },
    "flume/aws_access_key": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "",
      "type": "string",
      "recipes": [

      ],
      "display_name": "AWS access key used for writing to s3 buckets",
      "description": "AWS access key used for writing to s3 buckets"
    },
    "flume/aws_secret_key": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "",
      "type": "string",
      "recipes": [

      ],
      "display_name": "AWS secret key used for writing to s3 buckets",
      "description": "AWS secret key used for writing to s3 buckets"
    },
    "flume/collector": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "",
      "type": "string",
      "recipes": [

      ],
      "display_name": "Format of node's logs",
      "description": "output_format -- Controls what format the node writes logs (using collectorSink):\n * avro - Avro Native file format. Default currently is uncompressed.\n * avrodata - Binary encoded data written in the avro binary format.\n * avrojson - JSON encoded data generated by avro.\n * default - a debugging format.\n * json - JSON encoded data.\n * log4j - a log4j pattern similar to that used by CDH output pattern.\n * raw - Event body only. This is most similar to copying a file but does not preserve any uniqifying metadata like host/timestamp/nanos.\n * syslog - a syslog like text output format.\n\ncodec -- Controls what kind of compression the collector will use when writing a file.\nwhether or not collected logs are gzipped before writing\nthem to their final resting place (using collectorSink)\n * GZipCodec\n * BZip2Codec\n"
    },
    "flume/java_opts": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "",
      "type": "string",
      "recipes": [

      ],
      "display_name": "list of command line parameters to add to the jvm",
      "description": "list of command line parameters to add to the jvm"
    },
    "flume/master/external_zookeeper": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "",
      "type": "string",
      "recipes": [

      ],
      "display_name": "Use External Zookeeper?",
      "description": "false to use flume's zookeeper. True to attach to an external zookeeper."
    },
    "flume/classpath": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "",
      "type": "string",
      "recipes": [

      ],
      "display_name": "list of directories and jars to add to the FLUME_CLASSPATH",
      "description": "list of directories and jars to add to the FLUME_CLASSPATH"
    },
    "flume/classes": {
      "required": "optional",
      "calculated": false,
      "choice": [

      ],
      "default": "",
      "type": "string",
      "recipes": [

      ],
      "display_name": "",
      "description": ""
    }
  },
  "license": "Apache 2.0",
  "suggestions": {
  },
  "platforms": {
    "debian": ">= 0.0.0",
    "ubuntu": ">= 0.0.0"
  },
  "maintainer": "Chris Howe - Infochimps, Inc",
  "long_description": "# flume chef cookbook\n\nInstalls/Configures flume\n\n## Overview\n\nCookbook to install flume on a cluster.\n\nUse flume::master to set up a master node. Use flume::node to set up a\nphysical node. Currently only one physical node per machines. \n\nConfigure logical nodes with the logical_node resource - see the test_flow.rb \nrecipe for an example. This is still somewhat experimental, and some features\nwill not work as well as they should until chef version 0.9.14 and others until\nthe next release of flume.\n\nComing soon flume::xxx_plugin.\n\n#### Notes\n\nThis recipe relies on cluster_discovery_services to determine which nodes \nacross the cluster act as flume masters, and which nodes provide zookeeper\nservers.\n\n## Attributes\n\n* `[:flume][:aws_access_key]`         - AWS access key used for writing to s3 buckets\n* `[:flume][:aws_secret_key]`         - AWS secret key used for writing to s3 buckets\n* `[:flume][:cluster_name]`           -  (default: \"cluster_name\")\n  The name of the cluster to participate with (masters and zookeepers...)\n* `[:flume][:plugins]`                - Hash for plugin configuration\n  If you have a particular plugin to configure, you can also configure the classpath and the classes to include in the configuration file with attributes in the following forms:\n  node[:flume][:plugin][{plugin_name}][:classes]\n  node[:flume][:plugin][{plugin_name}][:classpath]\n  node[:flume][:plugin][{plugin_name}][:java_opts]\n* `[:flume][:classes]`                - \n* `[:flume][:classpath]`              - list of directories and jars to add to the FLUME_CLASSPATH\n* `[:flume][:java_opts]`              - list of command line parameters to add to the jvm\n* `[:flume][:collector]`              - Format of node's logs\n  output_format -- Controls what format the node writes logs (using collectorSink):\n   * avro - Avro Native file format. Default currently is uncompressed.\n   * avrodata - Binary encoded data written in the avro binary format.\n   * avrojson - JSON encoded data generated by avro.\n   * default - a debugging format.\n   * json - JSON encoded data.\n   * log4j - a log4j pattern similar to that used by CDH output pattern.\n   * raw - Event body only. This is most similar to copying a file but does not preserve any uniqifying metadata like host/timestamp/nanos.\n   * syslog - a syslog like text output format.\n  \n  codec -- Controls what kind of compression the collector will use when writing a file.\n  whether or not collected logs are gzipped before writing\n  them to their final resting place (using collectorSink)\n   * GZipCodec\n   * BZip2Codec\n  \n* `[:flume][:data_dir]`               - Directory for local in-transit files (default: \"/data/db/flume\")\n* `[:flume][:master][:external_zookeeper]` - Use External Zookeeper?\n  false to use flume's zookeeper. True to attach to an external zookeeper.\n* `[:flume][:master][:zookeeper_port]` - port to talk to zookeeper on (for external zookeeper) (default: \"2181\")\n\n## Recipes \n\n* `default`                  - Base configuration for flume\n* `hbase_sink_plugin`        - Hbase Sink Plugin\n* `jruby_plugin`             - Jruby Plugin\n* `master`                   - Configures Flume Master, installs and starts service\n* `node`                     - Configures Flume Node, installs and starts service\n* `test_flow`                - Test Flow\n\n\n## Integration\n\nSupports platforms: debian and ubuntu\n\nCookbook dependencies:\n* java\n* apt\n* mountable_volumes\n* provides_service\n\n\n## License and Author\n\nAuthor::                Chris Howe - Infochimps, Inc (<coders@infochimps.com>)\nCopyright::             2011, Chris Howe - Infochimps, Inc\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n> readme generated by [cluster_chef](http://github.com/infochimps/cluster_chef)'s cookbook_munger\n",
  "version": "3.0.0",
  "recommendations": {
  },
  "recipes": {
    "flume::test_flow": "Test Flow",
    "flume::node": "Configures Flume Node, installs and starts service",
    "flume::master": "Configures Flume Master, installs and starts service",
    "flume::jruby_plugin": "Jruby Plugin",
    "flume::default": "Base configuration for flume",
    "flume::hbase_sink_plugin": "Hbase Sink Plugin"
  },
  "groupings": {
  },
  "conflicting": {
  },
  "replacing": {
  },
  "description": "Installs/Configures flume",
  "providing": {
  }
}